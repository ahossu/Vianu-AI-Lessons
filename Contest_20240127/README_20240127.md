Statement

Train a Linear Regression on the following data and create at least 2 visualizations; add as many features as you can and be creative! Also, consider the dataset's features which can help you get the highest accuracy.
____________________

Use the following tutorials for improving the data analysis and also the linear regression:
Matplotlib [https://www.w3schools.com/python/matplotlib_intro.asp]
Matplotlib [https://matplotlib.org/stable/tutorials/pyplot.html]
Seaborn [https://seaborn.pydata.org/tutorial/introduction.html]
LinearRegression [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html]
Heatmap [https://seaborn.pydata.org/generated/seaborn.heatmap.html]
Multiple Linear Regression [https://www.w3schools.com/python/python_ml_multiple_regression.asp]
____________________

Dataset

DESCRIPTION:
The Student Performance Dataset is designed to examine the factors influencing academic student performance. The dataset consists of 10,000 student records, each containing information about various predictors and a performance index.
Variables:
Hours Studied: The total number of hours spent studying by each student.
Previous Scores: The scores obtained by students in previous tests.
Extracurricular Activities: Whether the student participates in extracurricular activities (Yes or No).
Sleep Hours: The average number of hours of sleep the student had per day.
Sample Question Papers Practiced: The number of sample question papers the student practised.
Target Variable:
Performance Index: A measure of the overall performance of each student. The performance index represents the student's academic performance and has been rounded to the nearest integer. The index ranges from 10 to 100, with higher values indicating better performance.
The dataset aims to provide insights into the relationship between the predictor variables and the performance index. Researchers and data analysts can use this dataset to explore the impact of studying hours, previous scores, extracurricular activities, sleep hours, and sample question papers on student performance.

P.S.: Please note that this dataset is synthetic and created for illustrative purposes. The relationships between the variables and the performance index may not reflect real-world scenarios

Dataset's license:
Anyone is free to share and use the data
____________________

Linear Regression
Definition: Linear regression is a statistical method that models the relationship between a dependent variable and one or more independent variables using a linear equation. The model assumes that the change in the dependent variable is a linear function of the change in the independent variables.

When to Use: Use linear regression when you want to predict a continuous outcome based on one or more predictor variables. Itâ€™s best suited for situations where there is a linear relationship between the independent and dependent variables.

Logistic Regression
Definition: Logistic regression is a statistical model used to predict the probability of a binary outcome (1/0, True/False) based on one or more predictor variables. It uses the logistic function to model the probabilities.

When to Use: Use logistic regression when you need to classify data into two distinct categories. It's commonly used in binary classification problems such as spam detection, medical diagnosis (disease vs. no disease), and binary decision-making processes.

Decision Tree
Definition: A decision tree is a flowchart-like model that makes decisions by splitting the data into branches based on the value of input features. Each node represents a feature, each branch represents a decision rule, and each leaf node represents an outcome.

When to Use: Decision trees are useful for both classification and regression tasks. They are easy to interpret and understand, making them ideal for problems where model transparency is important. They handle both numerical and categorical data well.

K-Nearest Neighbors (KNN)
Definition: K-Nearest Neighbors is a non-parametric, lazy learning algorithm used for classification and regression. It makes predictions by finding the K most similar instances (neighbors) in the training data and taking a majority vote (for classification) or averaging the values (for regression).

When to Use: Use KNN when you have a small to medium-sized dataset and the decision boundaries are not linear. It is effective for problems where similar instances have similar outcomes, such as image recognition and recommendation systems.

Convolutional Neural Network (CNN)
Definition: A Convolutional Neural Network is a type of deep learning model specifically designed for processing structured grid data like images. CNNs use convolutional layers to automatically and adaptively learn spatial hierarchies of features from input images.

When to Use: Use CNNs for image-related tasks such as image classification, object detection, and image segmentation. They are also effective for other types of spatial data, like video processing and certain types of time-series data.

Neural Network (NN)
Definition: A Neural Network is a series of algorithms that attempt to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. It consists of interconnected nodes (neurons) organized in layers.

When to Use: Use neural networks for a wide range of applications, including classification, regression, and clustering tasks. They are particularly powerful for problems with large and complex datasets where traditional models may fall short, such as speech recognition, natural language processing, and complex pattern recognition.

Support Vector Machine (SVM)
Definition: Support Vector Machine is a supervised learning algorithm used for classification and regression tasks. It finds the hyperplane that best separates the classes in the feature space, maximizing the margin between the closest points of the classes (support vectors).

When to Use: Use SVM for binary classification tasks with small to medium-sized datasets. It is effective in high-dimensional spaces and is commonly used in text classification, image recognition, and bioinformatics.

Random Forest
Definition: Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (for classification) or mean prediction (for regression) of the individual trees.

When to Use: Use random forests for both classification and regression tasks when you need a robust and accurate model that can handle large datasets with a high number of features. It is particularly useful when you want to reduce overfitting.

Gradient Boosting Machine (GBM)
Definition: Gradient Boosting Machine is an ensemble technique that builds models sequentially. Each new model attempts to correct the errors made by the previous models. It uses gradient descent to minimize the loss function.

When to Use: Use GBM for high-performance tasks where you need to optimize prediction accuracy. It is particularly effective for structured/tabular data in problems like credit scoring, anomaly detection, and other classification and regression tasks.

Principal Component Analysis (PCA)
Definition: Principal Component Analysis is a dimensionality reduction technique that transforms a large set of variables into a smaller one by finding new orthogonal axes (principal components) that capture the most variance in the data.

When to Use: Use PCA when you need to reduce the dimensionality of your data to simplify models, reduce computation time, or eliminate multicollinearity. It is often used in exploratory data analysis and for pre-processing in machine learning pipelines.

Each of these models has its own strengths and is suited to different types of problems. The choice of model depends on the specific characteristics of your data and the problem you are trying to solve.
____________________

Linear Regression

from sklearn.linear_model import LinearRegression
import numpy as np

# Example data
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
y = np.dot(X, np.array([1, 2])) + 3

# Initialize and fit the model
model = LinearRegression().fit(X, y)

# Predict
predictions = model.predict(np.array([[3, 5]]))
print(predictions)
____________________

Logistic Regression

from sklearn.linear_model import LogisticRegression

# Example data
X = [[0, 0], [1, 1], [2, 2], [3, 3]]
y = [0, 0, 1, 1]

# Initialize and fit the model
model = LogisticRegression().fit(X, y)

# Predict
predictions = model.predict([[1.5, 1.5]])
print(predictions)
____________________

Decision Tree

from sklearn.tree import DecisionTreeClassifier

# Example data
X = [[0, 0], [1, 1], [2, 2], [3, 3]]
y = [0, 0, 1, 1]

# Initialize and fit the model
model = DecisionTreeClassifier().fit(X, y)

# Predict
predictions = model.predict([[1.5, 1.5]])
print(predictions)
____________________

K-Nearest Neighbors (KNN)

from sklearn.neighbors import KNeighborsClassifier

# Example data
X = [[0, 0], [1, 1], [2, 2], [3, 3]]
y = [0, 0, 1, 1]

# Initialize and fit the model
model = KNeighborsClassifier(n_neighbors=3).fit(X, y)

# Predict
predictions = model.predict([[1.5, 1.5]])
print(predictions)
____________________

Convolutional Neural Network (CNN)

import tensorflow as tf
from tensorflow.keras import layers, models

# Example data (MNIST dataset)
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
X_train, X_test = X_train / 255.0, X_test / 255.0

# Build the model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Compile and train the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc}')
____________________

Neural Network (NN)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Example data
import numpy as np
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])  # XOR problem

# Build the model
model = Sequential()
model.add(Dense(10, input_dim=2, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile and train the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=1000, verbose=0)

# Predict
predictions = model.predict(X)
print(predictions)
____________________

Support Vector Machine (SVM)

from sklearn import svm

# Example data
X = [[0, 0], [1, 1], [2, 2], [3, 3]]
y = [0, 0, 1, 1]

# Initialize and fit the model
model = svm.SVC().fit(X, y)

# Predict
predictions = model.predict([[1.5, 1.5]])
print(predictions)
____________________

Random Forest

from sklearn.ensemble import RandomForestClassifier

# Example data
X = [[0, 0], [1, 1], [2, 2], [3, 3]]
y = [0, 0, 1, 1]

# Initialize and fit the model
model = RandomForestClassifier(n_estimators=10).fit(X, y)

# Predict
predictions = model.predict([[1.5, 1.5]])
print(predictions)
____________________

Gradient Boosting Machine (GBM)

from sklearn.ensemble import GradientBoostingClassifier

# Example data
X = [[0, 0], [1, 1], [2, 2], [3, 3]]
y = [0, 0, 1, 1]

# Initialize and fit the model
model = GradientBoostingClassifier().fit(X, y)

# Predict
predictions = model.predict([[1.5, 1.5]])
print(predictions)
____________________

Principal Component Analysis (PCA)

from sklearn.decomposition import PCA
import numpy as np

# Example data
X = np.array([[2.5, 2.4],
              [0.5, 0.7],
              [2.2, 2.9],
              [1.9, 2.2],
              [3.1, 3.0],
              [2.3, 2.7],
              [2, 1.6],
              [1, 1.1],
              [1.5, 1.6],
              [1.1, 0.9]])

# Initialize and fit the model
pca = PCA(n_components=1)
principalComponents = pca.fit_transform(X)

print(principalComponents)
____________________

1. VGG (Visual Geometry Group)
Description: VGG models are deep convolutional neural networks proposed by the Visual Geometry Group at the University of Oxford.
Usage:
How to Use: Load the pre-trained model using TensorFlow or PyTorch libraries. For example, in TensorFlow: tf.keras.applications.VGG16(weights='imagenet').
When to Use: VGG models are suitable for image classification tasks where higher accuracy is required. They are also useful as feature extractors in transfer learning scenarios.
2. ResNet (Residual Network)
Description: ResNet is a deep residual learning framework that won the ImageNet Large Scale Visual Recognition Challenge in 2015.
Usage:
How to Use: Load the pre-trained model using TensorFlow or PyTorch libraries. For example, in PyTorch: torchvision.models.resnet18(pretrained=True).
When to Use: ResNet models are effective for tasks such as image classification, object detection, and image segmentation. They excel in scenarios where deep networks are needed without suffering from the vanishing gradient problem.
3. BERT (Bidirectional Encoder Representations from Transformers)
Description: BERT is a transformer-based model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context.
Usage:
How to Use: Load the pre-trained model using the Hugging Face transformers library. For example: from transformers import BertModel, BertTokenizer; model = BertModel.from_pretrained('bert-base-uncased').
When to Use: BERT is widely used for natural language understanding tasks such as sentiment analysis, text classification, question answering, and named entity recognition. It excels in tasks requiring contextual understanding of language.
4. GPT (Generative Pre-trained Transformer)
Description: GPT models are based on the transformer architecture and are pre-trained using unsupervised learning on large corpora of text data.
Usage:
How to Use: Load the pre-trained model using the transformers library. For example: from transformers import GPT2Model, GPT2Tokenizer; model = GPT2Model.from_pretrained('gpt2').
When to Use: GPT models are useful for text generation tasks, language modeling, and other tasks where generating coherent text output is required. They are also used in conversational AI applications.
5. YOLO (You Only Look Once)
Description: YOLO is a state-of-the-art, real-time object detection system.
Usage:
How to Use: You can use pre-trained weights with frameworks like Darknet or PyTorch. For example, with PyTorch: model = torch.hub.load('ultralytics/yolov5', 'yolov5s').
When to Use: YOLO models are ideal for applications requiring real-time object detection, such as autonomous vehicles, surveillance, and robotics.
6. Inception
Description: Inception models (like InceptionV3) are deep convolutional networks introduced by Google Research.
Usage:
How to Use: Load the pre-trained model using TensorFlow or Keras. For example: from tensorflow.keras.applications import InceptionV3; model = InceptionV3(weights='imagenet').
When to Use: Inception models are suitable for image classification and object recognition tasks. They are known for their efficient use of computational resources and multi-scale feature extraction.
7. MobileNet
Description: MobileNets are efficient neural network models designed for mobile and embedded vision applications.
Usage:
How to Use: Load the pre-trained model using TensorFlow or Keras. For example: from tensorflow.keras.applications import MobileNetV2; model = MobileNetV2(weights='imagenet').
When to Use: MobileNets are used in scenarios where computational resources are limited, such as mobile devices or embedded systems, without sacrificing too much accuracy in image classification tasks.
8. Xception
Description: Xception is an extension of the Inception architecture, where depthwise separable convolutions are used to improve model efficiency.
Usage:
How to Use: Load the pre-trained model using TensorFlow or Keras. For example: from tensorflow.keras.applications import Xception; model = Xception(weights='imagenet').
When to Use: Xception models are useful for image classification tasks where high accuracy is required, similar to Inception models but with potentially better efficiency.
These pre-trained models provide powerful tools for various machine learning and deep learning tasks, spanning from computer vision to natural language processing. They are readily available and can be integrated into your projects with ease, leveraging the expertise of the research community and the efficiency of pre-training on large datasets.
____________________

1. VGG (Visual Geometry Group)

# Example using VGG16 in TensorFlow/Keras
import tensorflow as tf
from tensorflow.keras.applications import VGG16, preprocess_input, decode_predictions
import matplotlib.pyplot as plt
import numpy as np

# Load the pre-trained model
model = VGG16(weights='imagenet')

# Load and preprocess an example image
img_path = 'example.jpg'
img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))
x = tf.keras.preprocessing.image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

# Predict
preds = model.predict(x)
decoded_preds = decode_predictions(preds, top=3)[0]  # Get top 3 predictions

# Print predictions
print('Predictions:')
for pred in decoded_preds:
    print(f'{pred[1]}: {pred[2]:.4f}')

# Display the image
plt.imshow(img)
plt.axis('off')
plt.show()
____________________

2. BERT (Bidirectional Encoder Representations from Transformers)

# Example using BERT in Hugging Face Transformers
from transformers import BertTokenizer, BertModel
import torch

# Load pre-trained model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Tokenize input text
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

# Forward pass through the model
outputs = model(**inputs)

# Access the pooled output (embedding) of the first token
pooled_output = outputs.pooler_output

# Print shape of pooled_output
print(f'Shape of pooled_output: {pooled_output.shape}')

# Perform tasks like sentiment analysis, text classification, etc., using pooled_output
____________________

3. YOLO (You Only Look Once)

# Example using YOLOv5 with PyTorch
import torch
from PIL import Image

# Load the model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Load an image
img = Image.open('example.jpg')

# Perform inference
results = model(img)

# Display results
results.show()

# Access objects detected and their details
print('Detected Objects:')
for obj in results.xyxy[0]:
    print(f'{obj[-1]} - Confidence: {obj[-2]}')
____________________

4. Inception

# Example using InceptionV3 in TensorFlow/Keras
import tensorflow as tf
from tensorflow.keras.applications import InceptionV3, decode_predictions, preprocess_input
import matplotlib.pyplot as plt
import numpy as np

# Load the pre-trained model
model = InceptionV3(weights='imagenet')

# Load and preprocess an example image
img_path = 'example.jpg'
img = tf.keras.preprocessing.image.load_img(img_path, target_size=(299, 299))
x = tf.keras.preprocessing.image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

# Predict
preds = model.predict(x)
decoded_preds = decode_predictions(preds, top=3)[0]  # Get top 3 predictions

# Print predictions
print('Predictions:')
for pred in decoded_preds:
    print(f'{pred[1]}: {pred[2]:.4f}')

# Display the image
plt.imshow(img)
plt.axis('off')
plt.show()