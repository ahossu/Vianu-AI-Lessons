{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18437e7-1ea2-4cf5-aa1f-421138fe90d5",
   "metadata": {},
   "source": [
    "## Reinforcement Learning!\n",
    "\n",
    "In lectia de astazi vom vorbi despre reinforcement learning si in acest notebook vom implementa un program care sa ajute o racheta cu 3 propulsoare (stanga, dreapta si mijloc) sa aterizeze pe o suprafata denivelata. Astfel, vom avea 4 actiuni posibile: sa nu facem nimic, sa activam propulsorul din stanga, cel din dreapta sau cel din mijloc.\n",
    "\n",
    "Acest algoritm se bazeaza pe o abordare foarte intuitiva: vom stabili un set de rewarduri si pedepse pentru program, iar mereu cand face o actiune buna il rasplatim, iar cand face o actiune negativa il vom pedepsi. Astfel vom nota aceasta functie cu R(s), unde \"s\" este state-ul in care ne aflam. State-ul in acest caz va fi descris de urmatoarele valori: pozitiile pe axele x si y, vitezele pe aceste axe, unghiul rachetei, viteza unghiulara, daca piciorusul stang, respectiv drept al rachetei atinge pamantul sau nu.\n",
    "\n",
    "De asemenea, putem defini o alta functie ajutatoare, Q(s, a) care ne da scorul pentru state-ul \"s\" in caz ca facem actiunea \"a\" daca dupa aceasta actiune ne vom comporta optim. Astfel, formula va fi Q(s, a) = R(s) + max(Q(s2, a')), unde \"s\" este stateul curent, \"a\" actiunea pe care vrem sa o facem, \"s2\" state-ul in care vom ajunge dupa ce facem actiunea \"a\", iar max(Q(s2, a')) reprezentand valoarea maxima posibila a functiei Q pentru stateul s2.\n",
    "\n",
    "Pentru ca scopul nostru in reinforcement learning este sa realizam un task in cel mai scurt timp posibil, vom adauga un discount factor \"γ\" care va scade treptat scorul cu cat trece timpul. Pentru a exprima asta in mod matematic, vom da valori lui γ mai mici ca 1 si vom scrie functia Q ca o functie polinomiala in functie de γ: Q(s, a) = R(s) + γ * Q(s2, a') + γ^2 * Q(s3, a'') + γ^3 * Q(s4, a''') + ...\n",
    "Daca dam factor comun γ, observam ca formula initiala devine Q(s, a) = R(s) + γ * max(Q(s2, a')).\n",
    "\n",
    "Pare ca avem toate elementele necesare: pur si simplu trebuie la fiecare state sa vedem actiunea care ne aduce cel mai mare scor cu ajutorul functiei Q si actionam corespunzator. Problema noastra este urmatoarea: cum calculam functia Q? Momentam avem doar o exprimare a functiei Q in functie de alte valori ale functiei, asa ca nu prea ne ajuta. Astfel, ne putem gandi la o retea neuronala care sa aproximeze aceasta valoare. Daca antrenam una care sa ne mimeze functia Q?\n",
    "\n",
    "Trainingul retelei va fi ceva in genul: vom tine minte un buffer de stateuri, actiuni, rewarduri etc si vom forma un dataset.\n",
    "X-urile vor fi reprezentate de tuple-uri (s, a), iar Y-urile vor fi R(s) + γ * max(Q(s2, a')). Astfel, vom avea reteaua neuronala Q care va fi functia noastra si target_Q care va fi o retea identica auxiliara. La un pas de train vom antrena reteaua target_Q pe dataset, apoi vom transfera ce am invatat in reteaua noastra Q folosind soft update, adica preluam doar un procentaj din reteaua noua in cea de baza (Q) pentru a nu uita comportamente vechi.\n",
    "\n",
    "Totusi, asta nu e tot. Pentru a incuraja modelul sa exploreze diferite approachuri, vom include un element de randomizare a actiunilor numit epsilon-greedy policy: la fiecare pas in care trebuie sa facem o actiune avem o sansa de a alege o actiune random si o sansa de a alege o actiune conform modelului nostru (epsilon). De asemenea, acest epsilon va scade treptat, odata ce ne perfectionam modelul."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf87428",
   "metadata": {},
   "source": [
    "#### Cum sa instalam biblioteciile\n",
    "\n",
    "- pip install numpy\n",
    "- pip install tensorflow\n",
    "- pip install gym\n",
    "- pip install box2d pygame\n",
    "- pip install box2d-kengz\n",
    "- pip install imageio ipython -> trebuie sa instalam ffmpeg (brew install ffmpeg pentru macos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968252e-b9c9-4641-a634-7da1c477911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial includem toate bibliotecile necesare\n",
    "import time\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# necesare pentru procesare si retele neuronale:\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# necesare pentru simulator:\n",
    "import gym\n",
    "\n",
    "# necesare pentru video\n",
    "import os\n",
    "os.environ[\"IMAGEIO_FFMPEG_EXE\"] = '/opt/homebrew/bin/ffmpeg' # pathul unde e instalat ffmpeg\n",
    "import imageio\n",
    "import IPython\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d87005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dam setup simularii\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.reset()\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54fc095",
   "metadata": {},
   "source": [
    "### Structura retelei\n",
    "\n",
    "Initial ne-am gandi la o structura simpla: un input layer cu 12 neuroni (8 valori care descriu stateul, 4 valori care reprezinta one-hot encodingul unei actiuni), niste hidden layers si un output layer cu un neuron care sa ne dea valoarea Q(s, a).\n",
    "\n",
    "Totusi, aceasta structura este putin ineficienta, pentru ca noi mereu vrem sa stim max(Q(s, a)), astfel vom apela modelul de 4 ori, ceea ce e foarte lent. Asadar, vom inlocui ultimul layer cu un neuron cu unul cu 4, pentru a avea mereu toate valorile la o singura apelare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1493972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrii\n",
    "MEMORY_SIZE = 100000 # ultimele 100k actiuni\n",
    "GAMMA = 0.995 # discount factor\n",
    "ALPHA = 1e-3 # learning rate\n",
    "NUM_STEPS_FOR_UPDATE = 4 # modelul invata din 4 in 4 pasi de simulare\n",
    "\n",
    "SEED = 0 # seed pentru initializare parametrilor random ai modelului\n",
    "MINIBATCH_SIZE = 64\n",
    "TAU = 1e-3 # procentaj pentru soft update\n",
    "E_DECAY = 0.995 # rata la care scade ε pentru ε-greedy policy.\n",
    "E_MIN = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337593c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retelele neuronale\n",
    "q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=num_actions, activation='linear'), \n",
    "])\n",
    "\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=num_actions, activation='linear'),\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # max target_Q(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    \n",
    "    # y = R daca se termina episodul de invatare, altfel y = R + γ max Q^(s,a).\n",
    "    y_targets = rewards + gamma * max_qsa * (1 - done_vals)\n",
    "    \n",
    "    # luam valorile din modelul Q si le punem in aceeasi forma ca y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]), tf.cast(actions, tf.int32)], axis=1))\n",
    "    \n",
    "    loss = MSE(y_targets, q_values)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft update: luam p% din reteaua curenta si (100 - p)% din noua retea pentru stabilizare si pentru a nu uita anumite comportamente\n",
    "def update_target_network(q_network, target_q_network):\n",
    "    for target_weights, q_net_weights in zip(target_q_network.weights, q_network.weights):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functia in care dam update retelei\n",
    "\n",
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    # calculam loss-ul\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # luam derivatele din model\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # dam update parametrilor retelei Q\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # dam update parametrilor retelei target_Q cu ajutorul soft update\n",
    "    update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functia care ne da actiunea cea mai buna bazata pe rezultatele date de retea si epsilon-greedy policy\n",
    "\n",
    "def get_action(q_values, epsilon=0.0):\n",
    "    if random.random() > epsilon:\n",
    "        return np.argmax(q_values.numpy()[0])\n",
    "    else:\n",
    "        return random.choice(np.arange(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie ajutatoare care ne spune daca este timpul sa dam update retelei target_Q\n",
    "\n",
    "def check_update_conditions(t, num_steps_upd, memory_buffer):\n",
    "    if (t + 1) % num_steps_upd == 0 and len(memory_buffer) > MINIBATCH_SIZE:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie care ne da un minibatch de date random din bufferul de actiuni\n",
    "\n",
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=MINIBATCH_SIZE)\n",
    "    states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]), dtype=tf.float32)\n",
    "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8), dtype=tf.float32)\n",
    "    return (states, actions, rewards, next_states, done_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc89c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie care ne scade treptat epsilonul\n",
    "\n",
    "def get_new_eps(epsilon):\n",
    "    return max(E_MIN, E_DECAY * epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucla de training\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100 # cate experiente sa folosim pentru a vedea media punctajului in timpul trainingului\n",
    "epsilon = 1.0\n",
    "\n",
    "# facem bufferul de memorie\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "# setam aceleasi valori pentru parametrii si in a doua retea\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # resetam simularea si luam state-ul\n",
    "    state = env.reset()\n",
    "    state = list(state)[0]\n",
    "    total_points = 0\n",
    "    \n",
    "    for t in range(max_num_timesteps):\n",
    "        # pentru acest state luam o actiune folosing epsilon-greedy policy\n",
    "        state_qn = np.expand_dims(state, axis=0)  # modificam dimensiunile state-ului pentru a-l putea trimite in retea\n",
    "        q_values = q_network(state_qn)\n",
    "        action = get_action(q_values, epsilon)\n",
    "        \n",
    "        # facem actiunea corespunzatoare si luam feedback-ul de la simulator si state-ul urmator\n",
    "        next_state, reward, done, _, __ = env.step(action)\n",
    "        next_state = np.array(next_state)\n",
    "        \n",
    "        # tinem minte aceasta experienta in memorie\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        # vedem daca e timpul de un update\n",
    "        update = check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "            # luam un minibatch din buffer\n",
    "            experiences = get_experiences(memory_buffer)\n",
    "            \n",
    "            # facem un pas de gradient descent si dam update la retea\n",
    "            agent_learn(experiences, GAMMA)\n",
    "        \n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    # actualizam epsilon\n",
    "    epsilon = get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisodul {i+1} | Media scorului ultimelor {num_p_av} de episoade: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisodul {i+1} | Media scorului ultimelor {num_p_av} de episoade: {av_latest_points:.2f}\")\n",
    "\n",
    "    # consideram ca am rezolvat problema daca avem o medie de cel putin 200 de puncte in ultimul timp\n",
    "    if av_latest_points >= 200.0:\n",
    "        print(f\"\\n\\nProblema rezolvata in {i+1} episoade de trainig!\")\n",
    "        q_network.save('lunar_lander_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTimp in care a rulat: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683baed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pentru a vedea comportamentul rachetei:\n",
    "\n",
    "def embed_mp4(filename):\n",
    "    video = open(filename, \"rb\").read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = \"\"\"\n",
    "    <video width=\"840\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>\"\"\".format(\n",
    "        b64.decode()\n",
    "    )\n",
    "\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "\n",
    "def create_video(filename, env, q_network, fps=30):\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        state = list(state)[0]\n",
    "        frame = env.render()\n",
    "        video.append_data(frame)\n",
    "        while not done:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            q_values = q_network(state)\n",
    "            action = np.argmax(q_values.numpy()[0])\n",
    "            state, _, done, _, __ = env.step(action)\n",
    "            frame = env.render()\n",
    "            video.append_data(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eaf611",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"lunar_lander.mp4\"\n",
    "\n",
    "create_video(filename, env, q_network)\n",
    "embed_mp4(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
